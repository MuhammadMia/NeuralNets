{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "metadata": {
    "interpreter": {
     "hash": "705abe06a65e8cc710549f31fafd994ab8ae41c68683faf5bae4d48d18f4c66e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Approaching Neural Networks\n",
    "\n",
    "## Inputs\n",
    "\n",
    "Represent every feature or parametre of a dataset as a matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = np.array([1, 5, 9])\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "source": [
    "## Hidden Layers\n",
    "\n",
    "Hidden Layers are a general term for all the layers between the input and output layers. There can be any number of hidden layers and any number of nodes within those layers. Outputs of a hidden layer typically go through an activation function before reaching the next layer. This normalizes their output values to within specific predefined ranges. \n",
    "\n",
    "## Weights\n",
    "\n",
    "Every node in one layer is connected to every node in the next. Each one of these connections has a weight. A measure of how relevant that connection is in the network. These are represented as a 2D array. Weights at first are randomly generated but there are other methods. \n",
    "\n",
    "## Biases\n",
    "\n",
    "Every node can have some bias. This is added to the incoming sum. The biases are also randomly generated on init. \n",
    "\n",
    "\n",
    "\n",
    "## Calculating the resultant matrix of Hidden Layer 1 (before adding biases)\n",
    "\n",
    "This is achieved via matrix multiplication of the inputs x the weights. There are 3 nodes in HL1 and 3 nodes in the input. 3 x 3 = 9 connections therefore 9 weights. \n",
    "\n",
    "We then perform a matrix addition of the biases array for layer 1 to the resultant array of the multiplication above. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# 2D array of weights\n",
    "weights1 = np.array([[0.6, -0.8, 0.3], [0.4, 0.1, -0.3], [0.2, 0.8, 0.9]])\n",
    "\n",
    "# Matrix multiplication of inputs and weights\n",
    "L01 = np.matmul(inputs, weights1)\n",
    "\n",
    "# Hidden Layer 1 Biases\n",
    "L01Biases = np.array([0.2, 0.1, 1])\n",
    "\n",
    "L01Result = np.add(L01, L01Biases)\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "source": [
    "## Activation functions\n",
    "\n",
    "There are a few activation functions to work with but the basic concept is that if you need an output value squished to within a particlular range then you use an activation function that does what you need. TanH reduces outputs to between -1 and 1, Sigmoid squishes to between 0 and 1, etc. We'll just use the sigmoid to start. The sigmoid function is defined below. \n",
    "\n",
    "It is important to note that this is not a compulsory step and highly depends on what you are trying to achieve."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "def sigmoid (x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "L01Result = [sigmoid(L01Result[0]), sigmoid(L01Result[1]), sigmoid(L01Result[2])]\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "source": [
    "And that's it for calculating a layers resultant matrix. Repeat for additional layers and modify for extra nodes. The activation function can be changed or removed entirely. \n",
    "\n",
    "## Now let's calculate the resultant matrix of the output layer.\n",
    "\n",
    "We will have 2 outputs with no activation function. 3 nodes from the previous layer x 2 nodes here = 6 connections. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.49521249 0.80079981]\nComplete\n"
     ]
    }
   ],
   "source": [
    "weights2 = np.array([[0.5, -0.2], [0.2, 0.9], [-1.0, 1]])\n",
    "\n",
    "outputBiases = np.array([0.8, -0.9])\n",
    "\n",
    "output = np.matmul(L01Result, weights2)\n",
    "\n",
    "output = np.add(output, outputBiases)\n",
    "\n",
    "print(output)\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "source": [
    "# Training the network\n",
    "\n",
    "## Backpropogation\n",
    "\n",
    "Now that the network can arrive at a result, we need to train it to arrive at the right result. We can do this by comparing the result to a desired result, calculating the difference, and then going back through the network and adjusting the weights and biases to more closely resemble our desired outputs. This is called Backpropogation.\n",
    "\n",
    "Assume our desired outputs are 0, 1. We need to adjust the fixed values of our network (weights and biases) to get to that result rather than the result we've generated above. \n",
    "\n",
    "## Cost functions\n",
    "\n",
    "In order to determine how much and in which direction we need to make adjustments, we first need to define a cost function. Like activations, there are a few, but we will use MSE or mean-squared error. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.14245806580134726\nComplete\n"
     ]
    }
   ],
   "source": [
    "expOutput = np.array([0, 1])\n",
    "\n",
    "def mse (output, expOutput):\n",
    "    diffArr = np.subtract(output, expOutput)\n",
    "    sqArr = np.square(diffArr)\n",
    "\n",
    "    result = sqArr.mean()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "print(mse(output, expOutput))\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}